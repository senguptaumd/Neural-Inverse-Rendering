
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }
    h1 {
        font-weight:300;
    }
    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }
    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }
    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }
    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }
    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }
    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }
    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }
    hr
    {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(10, 10, 10, 0),
    rgba(10, 10, 10, 0.75), rgba(10, 10, 10, 0));
        margin: 1em 0 1em 0;
    }
</style>

<html>
  <head>
	  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116086767-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116086767-1');
</script>

        <title>Neural Inverse Rendering</title>
        <meta property="og:title" content="NIR" />	
  </head>

  <body>
    <br>
    <center>
      <span style="font-size:35px">Neural Inverse Rendering of an Indoor Scene From a Single Image
	</span>
    </center>

    <br><br>
      <table align=center width=900px>
       <tr>
         <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://homes.cs.washington.edu/~soumya91/"><sup>Soumyadip Sengupta</sup></a></span>
        </center>
        </td>
         
        <td align=center width=80px>
        <center>
        <span style="font-size:20px"><a href="https://www.gujinwei.org/"><sup>Jinwei Gu</sup></a></span>
        </center>
        </td>
         
         <td align=center width=80px>
        <center>
        <span style="font-size:25px"><a href="https://www.cc.gatech.edu/~kihwan23/"><sup>Kihwan Kim</sup></a></span>
        </center>
        </td>
	       
	<td align=center width=80px>
        <center>
        <span style="font-size:20px"><a href="https://liuguilin1225.github.io/"><sup>Guilin Liu</sup></a></span>
        </center>
        </td>

        <td align=center width=80px>
        <center>
        <span style="font-size:20px"><a href="https://www.cs.umd.edu/~djacobs/"><sup>David
		W. Jacobs</sup></a></span>
        </center>
        </td>
	       
	<td align=center width=80px>
        <center>
        <span style="font-size:20px"><a href="http://jankautz.com/"><sup>Jan Kautz</sup></a></span>
        </center>
        </td>

        
     </tr>
    </table>

    <br>
    <table align=center width=700px>
       <tr>
        <td align=center width=100px>
        <center>
		<span style="font-size:20px"><sup>NVIDIA</sup></span><br>
		<span style="font-size:20px"><sup>University of Maryland, College Park</sup></span><br>
		<span style="font-size:20px"><sup>University of Washington</sup><br> 
</span></center>
        </td>
     </tr>
    </table>

            <br>
            <table align=center width=1000px>
                <tr>
                    <td width=600px>
                      <center>
                          <a href="./resources/Teaser.png"><img src = "./resources/Teaser_new_single_rot.png" width="400px"></img></href></a><br>
                    </center>
                    </td>
                </tr>
                    <td width=700px>
                      <!-- <center> -->
                          <span style="font-size:14px"><i> <span style="font-weight:bold">Neural Inverse Rendering of an Indoor Scene From a Single Image.</span> We propose a self-supervised approach for inverse rendering.
We jointly decompose an indoor scene image into albedo,
surface normal and environment map lighting (top). Our method
outperforms state-of-the-art approaches (bottom) that solve for
only one of the scene attributes, i.e. albedo (Li et. al.), normal
(Zhang et. al.) and lighting (Gardner et. al.).</i>
                    <!-- </center> -->
                    </td>
                </tr>
            </table>

<br>
	    Inverse rendering aims to estimate physical attributes of a scene, e.g., reflectance, geometry, and lighting, from image(s). Inverse rendering has been studied primarily for
single objects or with methods that solve for only one of the scene attributes. We propose the first learning based approach
that jointly estimates albedo, normals, and lighting of an indoor scene from a single image. Our key contribution
is the Residual Appearance Renderer (RAR), which can be trained to synthesize complex appearance effects
(e.g., inter-reflection, cast shadows, near-field illumination, and realistic shading), which would be neglected otherwise.
This enables us to perform self-supervised learning on real data using a reconstruction loss, based on re-synthesizing
the input image from the estimated components. We finetune with real data after pretraining with synthetic data. Experimental
results show that our approach outperforms state-of-the-art methods that estimate one or more scene attributes.
            <br><br>
            <hr>
            <table align=center width=650>
              <center><h1>Paper</h1></center>
              <tr>
                <td style="padding:1em"><a href="https://arxiv.org/pdf/1901.02453.pdf"><img style="height:100px" src="./resources/networks_final.png"/></a></td>
                <td style="padding:1em"><span style="font-size:14pt">Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W. Jacobs, Jan Kautz.<br><br>
                    Neural Inverse Rendering of an Indoor Scene From a Single Image<br><br>
			In ICCV 2019.<br> </span>
                </td>
              </tr>
            </table>
            <br>

<table align=center width=180px>
              <tr>
                <td><span style="font-size:14pt"><center>
                      <a href="https://arxiv.org/pdf/1901.02453.pdf">[pdf]</a>
                </center></td>
		
                <td><span style="font-size:14pt"><center>
                      <a href="./resources/bibtex.txt">[Bibtex]</a>
                </center></td>
              </tr>
            </table>
            <br>  
	    <hr>
<!--             <center><h1>Code</h1></center>  -->
            <table align=center width=1000px> 
              <tr> 
		<center> 
		  <img class="round" style="width:800" src="./resources/networks_final.png"/> 
		</center>
              <td width=600px>
		<!-- <center> -->
		<span style="font-size:14px"><i> <span style="font-weight:bold">Overview of our approach.</span>  Our Inverse Rendering Network (IRN) predicts albedo, normals and illumination map. We train
on unlabeled real images using self-supervised reconstruction loss. Reconstruction loss consists of a closed-form Direct Renderer with no
learnable parameters and the proposed Residual Appearance Renderer (RAR), which learns to predict complex appearance effects.</i></span>
		<!-- </center> -->
              </td>
              </tr>
		    
<!-- 	      <tr>
	      <td><center> <br> 
		  <span style="font-size:25px">&nbsp;<a href='https://github.com/senguptaumd/SfSNet'>
			 [GitHub] </a> </span>
		  <br> 
		</center>
	      </td>
	      </tr> -->
	    </table>

<br>
<hr>

<a name="applications"></a>
			<center><h1>Results</h1></center>

			<br><br>
			<table align=center width=1100px>
				<tr>
					<td width=400px>
						<center> <span align="top" style="font-size:20px">Surface Normal Estimation</a></span><br><br> </center>
					</td>
					<td width=50px> </td>
					<td width=400px>
						<center> <span align="top" style="font-size:20px">Albedo Estimation</a></span><br><br> </center>
					</td>
				</tr>
				<tr height="300px">
					<td valign="top" height=400px>
					<center>
					<a href="./resourcesimages/normal.png"><img img src = "./resources/normal.png" height = "500px"></a><br>
						<span> Comparison with <a href="https://arxiv.org/abs/1612.07429">Zhang et. al.</a>
					<br><br>
					</center>
					</td>

					<td width=50px> </td>

					<td  valign="top" height=400px>
					<center>
					<a href="./resources/albedo.png"><img img src = "./resources/albedo.png" height = "500px"></a><br>
						<span> Comparison with <a href="http://www.cs.cornell.edu/projects/cgintrinsics/">Li et. al.</a>
					<br><br>
					</center>
					</td>
				</tr>
</table>

			<table align=center width=1100px>
				<tr>
					<td width=400px>
						<center> <span align="top" style="font-size:20px">Lighting Estimation</a></span><br><br> </center>
					</td>
					<td width=50px> </td>
					<td width=400px>
						<center> <span align="top" style="font-size:20px">More Results</a></span><br><br> </center>
					</td>
				</tr>
				<tr height="300px">
					<td valign="top" height=400px>
					<center>
					<a href="./resourcesimages/lighting.png"><img img src = "./resources/lighting.png" height = "250px"></a><br>
					<span> Comparison with <a href="http://vision.gel.ulaval.ca/~jflalonde/projects/deepIndoorLight/index.html">Gardner et. al.</a><br><br>
					</center>
					</td>

					<td width=50px> </td>

					<td  valign="top" height=400px>
					<center>
					<a href="./resources/more_Results.png"><img img src = "./resources/more_results.png" height = "350px"></a><br>
					<br><br>
					</center>
					</td>
				</tr>
</table>

		<center> <span align="top" style="font-size:20px">Role of RAR in albedo estimation</a></span><br><br> </center>
					<center>
					<a href="./resources/role_rar.png"><img img src = "./resources/role_rar.png" width = "800px"></a><br>
					</center>
					<center></center>


					
					
<br>
<center>
	For more qualitative and quantitative comparisons, please see our <a href="https://arxiv.org/pdf/1712.01261.pdf"> paper</a>.
</center>

<br>
<center>
	For downloading and visualizing more estimated results obtained by our algorithm: <a href="https://github.com/senguptaumd/Neural-Inverse-Rendering/tree/master/Sample"> please visit the following link</a>.
</center>

<center>
	For any additional questions or clarifications, please feel free to contact me (Soumyadip) at soumya91 @ cs.washington.edu</a>.
</center>


		<br>
		<hr>
            <table align=center width=1100px>
                <tr>
                    <td>
                      <left>
			<center><h1>Acknowledgements</h1></center>
			      We thank Hao Zhou and Chao Liu for helpful discussions. This research is partly supported by the National Science Foundation under grant no. IIS-1526234..			
		This webpage template is taken
			from <a href="https://shubhtuls.github.io/drc/">humans
			working on 3D</a> who borrowed it
		from some <a href="https://richzhang.github.io/colorization/">colorful folks</a>.
            </left>
        </td>
        </tr>
        </table>

        <br><br>
</body>
</html>
